{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka20bzp6PcNj"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/5_nb_supervised_learning.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swSaDjWvNDnY"
   },
   "source": [
    "# Chapter 5 - Algorithms for supervised learning \n",
    "The fifth chapter revisits two popular algorithms for supervised learning, the logistic regression model and classification and regression trees (CART). We examine their mathematical and statistical underpinnings to fully understand how logit and CART craft a model. Of course, we also exemplify how to use both algorithms in practice together with `sklearn`.  \n",
    "\n",
    "The outline of the tutorial is as follows:\n",
    "- Logistic regression\n",
    "  - The logistic function\n",
    "  - Maximum likelihood estimation\n",
    "  - Logistic regression from scratch\n",
    "  - Libraries for logistic regression\n",
    "- Decision trees\n",
    "  - Implementing a tree algorithm from scratch\n",
    "  - Decision trees with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjtJpOmnVf2K"
   },
   "source": [
    "# Logistic regression\n",
    "We learned from the lecture that the logistic regression model, or, in brief, logit model is the 'go-to' model for regression problems with a discrete target variable. Such problems are called classification problems. Formally speaking, logistic regression belongs to the wider family of **generalized linear models (GLM)**. It is a generalization of a linear regression model in that it embodies the well-known linear form $\\beta^{\\top} x$ and adds a (non-linear) **link function** that connects the linear form to the target variable. In the case of logistic regression, the link function is the logistic function. Other GLMs use different link functions, but these are out of scope. So let us first examine the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k68aGAQ_Vf2L"
   },
   "source": [
    "## The logistic function\n",
    "We implement the logistic function as a custom Python function and then generate some data to plot the function over the interval [-4, -3, ..., 4]. \n",
    "Recall that the form of the function for input variable $z$ is $f(z) = \\frac{1}{1 + \\exp^{-z}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "xSZq_kegVf2L",
    "outputId": "c5597ed2-5c41-41c6-c844-3724c014ea39"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we implement the logistic function as a custom function\n",
    "def logistic_fun(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "# create some data for plotting\n",
    "z = np.linspace(-4, 4)\n",
    "\n",
    "fz = logistic_fun(z)\n",
    "plt.title(\"The logistic function exemplified\", fontsize='small')\n",
    "plt.plot(z, fz);\n",
    "plt.plot(z, np.repeat(0.5, len(z)), \"--\");  #  just for fun, add a horizontal line to highlight 0.5\n",
    "plt.ylabel(\"$f(z)$\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QgT5fS6Vf2S"
   },
   "source": [
    "It is a good exercise to check how changes in the above implementation change the plot. For example, you could add a constant multiplier and calculate $e^{(-\\alpha z)} $ instead of $e^{(-z)}$, and check how the shape of the function changes with $\\alpha$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzDNTwuGVf2T"
   },
   "source": [
    "## Maximum likelihood estimation & gradient descent\n",
    "Estimating a logistic regression model involves minimizing the negative **log-likelihood** function. We discussed the maximum likelihood approach in the lecture. However, experience suggests that some students find this part a little hard  to digest, probably because of its level of formality. We will try to demystify the model by implementing it from scratch. For starters, however, recall that logistic regression models the probability of observing a certain state of our target variable $y$ by means of the logistic function. Assuming a zero-one coded binary target variable $y \\in {0, 1}$, we can write the logit model as follows:\n",
    "\n",
    "$$ p \\left( y=1|x \\right) = \\frac{1}{1+e^{-z}}, \\text{  and  }  p \\left( y=0|x \\right) = 1- p \\left( y=1|x \\right) $$\n",
    "\n",
    "We define $z$ as a linear, additive function of the features $x$: \n",
    "$$ z = \\hat{\\beta}^{\\top}x $$. \n",
    "\n",
    "Note that in the above equation, we assume for notational convenience that the feature vector $x$ includes a constant. This way, we do not need to explicitly consider the intercept. Assuming our data fulfills the IID assumption, we can write the log-likelihood function: \n",
    "\n",
    "$$ \\mathcal{L} = \\sum^{n}_{i=1} \\left[ y_i \\log \\left(p \\left( y=1|x \\right) \\right) + \\left(1-y_i\\right) \\log \\left(1-p \\left( y=1|x \\right) \\right)\\right]. $$\n",
    "\n",
    "Now we need to find the values of $ \\hat{\\beta} $ that minimize this function. To that end, we calculate the derivative of the log likelihood function with respect to $ \\hat{\\beta} $. We calculate this derivative using the chain rule:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\beta_j}} = \\frac{\\partial \\mathcal{L}}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\hat{\\beta}} $$\n",
    "\n",
    "Let's first calculate the three derivatives which we will need on the right hand side:\n",
    "\n",
    "\\begin{align*} \n",
    "\\frac{\\partial \\mathcal{L}}{\\partial p} &= \\frac{y}{p} - \\frac{1-y}{1-p} \\\\\n",
    "\\frac{\\partial p}{\\partial z} &= \\frac{1}{1+e^{-z}} \\cdot  \\Bigg( 1 - \\frac{1}{1+e^{-z}} \\Bigg) = \\Big( p \\cdot  \\big( 1 - p \\big) \\Big) \\\\\n",
    "\\frac{\\partial z}{\\partial \\hat{\\beta}} &= x\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Now we can multiply these three together to find the derivative of the log likelihood function with respect to $ \\hat{\\beta} $ :\n",
    " \n",
    "\\begin{align*}\n",
    " \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\beta}} &= \\frac{\\partial \\mathcal{L}}{\\partial p} \\cdot \\frac{\\partial p}{\\partial z} \\cdot \\frac{\\partial z}{\\partial \\hat{\\beta}} \\\\\n",
    " &= \\Big( \\frac{y}{p} - \\frac{1-y}{1-p} \\Big) \\cdot \\Big( p \\cdot  \\big( 1 - p \\big) \\Big) \\cdot x \\\\\n",
    " &= (y(1-p) - p(1-y)) \\cdot x \\\\ \n",
    " &= (y - p) \\cdot x\n",
    "\\end{align*}\n",
    "\n",
    "And voil√†! At the end, we have a much simpler formula to deal with:\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\beta}} = (y - p \\left( y=1|x \\right) ) \\cdot x $$\n",
    "\n",
    "The next step is to equate our derivate of zero and solve for the unknown parameter vector $\\hat{\\beta}$. However, different from linear regression, it turns out that there is no analytical solution. Therefore, we need to use another approach to find the optimal values of $\\hat{\\beta}$, that is the values that maximize the likelihood function.\n",
    "\n",
    "**Gradient descent** is the process of iteratively making changes to $ \\hat{\\beta} $ to assess which values lower the derivative value and, therefore, hopefully brings us closer to a minimum. Note that this may unfortunately not be the global minimum of the function but rather a local minimum. This is especially true if we have many coefficients or if any relationships are very complicated.\n",
    "\n",
    "Additionally, if we change $ \\hat{\\beta} $ by too much in one iteration, we may end up overshooting a minimum which is counterproductive. In order to avoid this, we multiply the total difference by the learning rate $ \\eta $ to dampen the update to $ \\hat{\\beta} $. If the learning rate is too low however, we may end up sabotaging ourselves as the function loses too much momentum in its updates. It is a good idea to experiment with a few different rates to see which one works best with your data.\n",
    "\n",
    "Here is an equation which represents gradient descent:\n",
    "$$ \\hat{\\beta}_j = \\hat{\\beta}_{j-1} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial \\hat{\\beta}_{j-1}} $$\n",
    "\n",
    "whereby we use the index $j$ to refer to individual iterations. For example, the above equation states how a new value of \n",
    "$\\hat{\\beta}_j$ depends on the value calculated in the previous iteration $\\hat{\\beta}_{j-1}$. Also note how our update rule embodies the value of our regression coefficients from the previous iteration with the gradient of our loss function $\\mathcal{L}$ with respect to $\\hat{\\beta}_{j-1}$. Essentially, the above equation says that, to update our regression parameters $\\hat{\\beta}$, take their values from the previous iteration and change these in the direction of the negative of the gradient, which is the direction in that our loss function decreases the most. The degree to which the new values of $\\hat{\\beta}$ depend on the previous values and the gradient, in other words the magnitude of the update, depend on the parameter $\\eta$, which is called the learning rate. \n",
    "\n",
    "For a more detailed explanation, check out the [Wikipedia page for *gradient descent*](https://en.wikipedia.org/wiki/Gradient_descent). It will be crucial for you to understand this process, so take advantage of online resources to improve your understanding. There are also many ways to modify this process like stochastic gradient descent or adaptive learning rates which you might come across in future studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUkvWfFsVf2m"
   },
   "source": [
    "## Libraries for logistic regression\n",
    "\n",
    "Now that we know that estimating a logistic regression model using the maximum likelihood approach is not some sort of dark magic, we might feel more comfortable with using libraries that hide all the details. In fact, you have already seen one such library in action in [Tutorial 3](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/3_nb_predictive_analytics.ipynb). There, we used the `statsmodels` library to estimate a logit model. Let's revisit the corresponding codes. However, doing so for our toy data is pointless. It's time for a real-world data set and since we spent so much time on cleaning and preparing and understanding the HMEQ data in [Tutorial 4](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/4_nb_data_preparation.ipynb), it would be a shame to not use it for demonstrating logistic regression on some real data. Remember that [Tutorial 4](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/4_nb_data_preparation.ipynb) discusses that data set in detail, so simply go back to the previous tutorial is you need a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkKRcRBRVf2n",
    "outputId": "c8664da3-b85a-458c-b162-823315d1ad0a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.tools import add_constant\n",
    "# Load data from GitHub\n",
    "df = pd.read_csv('../data/hmeq_modeling.csv', index_col=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PLQ7buLVf2q"
   },
   "source": [
    "Let's take a quick look at the data to re-familiarize ourselves with the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "zhgFxnX5Vf2s",
    "outputId": "7cc79eab-f3d4-4a59-9e0a-fdfbc73ff945"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "      <th>DEROGzero</th>\n",
       "      <th>REASON_HomeImp</th>\n",
       "      <th>REASON_IsMissing</th>\n",
       "      <th>JOB_Office</th>\n",
       "      <th>JOB_Other</th>\n",
       "      <th>JOB_ProfExe</th>\n",
       "      <th>JOB_Sales</th>\n",
       "      <th>JOB_Self</th>\n",
       "      <th>DELINQcat_1</th>\n",
       "      <th>DELINQcat_1+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.832283</td>\n",
       "      <td>-1.295882</td>\n",
       "      <td>-1.335526</td>\n",
       "      <td>0.266788</td>\n",
       "      <td>-1.075278</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-1.297476</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.810666</td>\n",
       "      <td>-0.013474</td>\n",
       "      <td>-0.672699</td>\n",
       "      <td>-0.236615</td>\n",
       "      <td>-0.723092</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.756608</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.789048</td>\n",
       "      <td>-1.654549</td>\n",
       "      <td>-1.839275</td>\n",
       "      <td>-0.668103</td>\n",
       "      <td>-0.368769</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-1.189302</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.789048</td>\n",
       "      <td>-0.159552</td>\n",
       "      <td>-0.202559</td>\n",
       "      <td>-0.236615</td>\n",
       "      <td>-0.061033</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-0.107566</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>-1.767431</td>\n",
       "      <td>0.791699</td>\n",
       "      <td>0.311107</td>\n",
       "      <td>-0.811933</td>\n",
       "      <td>-1.088528</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.756608</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BAD      LOAN   MORTDUE     VALUE       YOJ     CLAGE      NINQ  \\\n",
       "index                                                                      \n",
       "0       True -1.832283 -1.295882 -1.335526  0.266788 -1.075278 -0.065054   \n",
       "1       True -1.810666 -0.013474 -0.672699 -0.236615 -0.723092 -0.826792   \n",
       "2       True -1.789048 -1.654549 -1.839275 -0.668103 -0.368769 -0.065054   \n",
       "3       True -1.789048 -0.159552 -0.202559 -0.236615 -0.061033 -0.065054   \n",
       "4      False -1.767431  0.791699  0.311107 -0.811933 -1.088528 -0.826792   \n",
       "\n",
       "           CLNO   DEBTINC  DEROGzero  REASON_HomeImp  REASON_IsMissing  \\\n",
       "index                                                                    \n",
       "0     -1.297476  0.137456       True               1                 0   \n",
       "1     -0.756608  0.137456       True               1                 0   \n",
       "2     -1.189302  0.137456       True               1                 0   \n",
       "3     -0.107566  0.137456       True               0                 1   \n",
       "4     -0.756608  0.137456       True               1                 0   \n",
       "\n",
       "       JOB_Office  JOB_Other  JOB_ProfExe  JOB_Sales  JOB_Self  DELINQcat_1  \\\n",
       "index                                                                         \n",
       "0               0          1            0          0         0            0   \n",
       "1               0          1            0          0         0            0   \n",
       "2               0          1            0          0         0            0   \n",
       "3               0          1            0          0         0            0   \n",
       "4               1          0            0          0         0            0   \n",
       "\n",
       "       DELINQcat_1+  \n",
       "index                \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_b8UOuPVf2x",
    "outputId": "5228879e-1d62-46d5-917c-118f4b0c334d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.394606\n",
      "         Iterations 7\n"
     ]
    }
   ],
   "source": [
    "# Split into target and features\n",
    "y = df.BAD.values.astype(int)\n",
    "X = df.iloc[:,1:]\n",
    "\n",
    "# We add a constant column to X. Think of this as the Python way to include an intercept in your model \n",
    "X = add_constant(X, prepend=True, has_constant='raise')\n",
    "# Estimate logit model\n",
    "logit_model = sm.Logit(y, X.astype(float), ) \n",
    "logit_model = logit_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3_inlDcVf21"
   },
   "source": [
    "A nice feature of the `statsmodels` implementation is that you can easily produce a standard regression table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "C0k55AmTVf21",
    "outputId": "1304a381-233b-4c2f-d7c0-6825683a4c2d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>   <td>  5960</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  5941</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    18</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 04 Sep 2022</td> <th>  Pseudo R-squ.:     </th>   <td>0.2103</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:47:48</td>     <th>  Log-Likelihood:    </th>  <td> -2351.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -2978.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>5.480e-255</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "          <td></td>            <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>            <td>   -1.1279</td> <td>    0.134</td> <td>   -8.389</td> <td> 0.000</td> <td>   -1.391</td> <td>   -0.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>LOAN</th>             <td>   -0.1706</td> <td>    0.042</td> <td>   -4.018</td> <td> 0.000</td> <td>   -0.254</td> <td>   -0.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>MORTDUE</th>          <td>   -0.1583</td> <td>    0.067</td> <td>   -2.366</td> <td> 0.018</td> <td>   -0.289</td> <td>   -0.027</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>VALUE</th>            <td>    0.0619</td> <td>    0.070</td> <td>    0.885</td> <td> 0.376</td> <td>   -0.075</td> <td>    0.199</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YOJ</th>              <td>   -0.0619</td> <td>    0.041</td> <td>   -1.510</td> <td> 0.131</td> <td>   -0.142</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CLAGE</th>            <td>   -0.4503</td> <td>    0.043</td> <td>  -10.520</td> <td> 0.000</td> <td>   -0.534</td> <td>   -0.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>NINQ</th>             <td>    0.2806</td> <td>    0.035</td> <td>    7.944</td> <td> 0.000</td> <td>    0.211</td> <td>    0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CLNO</th>             <td>   -0.1448</td> <td>    0.042</td> <td>   -3.434</td> <td> 0.001</td> <td>   -0.227</td> <td>   -0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DEBTINC</th>          <td>    0.4379</td> <td>    0.044</td> <td>   10.033</td> <td> 0.000</td> <td>    0.352</td> <td>    0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DEROGzero</th>        <td>   -1.1282</td> <td>    0.097</td> <td>  -11.653</td> <td> 0.000</td> <td>   -1.318</td> <td>   -0.938</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REASON_HomeImp</th>   <td>    0.2465</td> <td>    0.086</td> <td>    2.878</td> <td> 0.004</td> <td>    0.079</td> <td>    0.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>REASON_IsMissing</th> <td>    0.3610</td> <td>    0.199</td> <td>    1.812</td> <td> 0.070</td> <td>   -0.030</td> <td>    0.751</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JOB_Office</th>       <td>   -0.6247</td> <td>    0.148</td> <td>   -4.219</td> <td> 0.000</td> <td>   -0.915</td> <td>   -0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JOB_Other</th>        <td>   -0.0035</td> <td>    0.115</td> <td>   -0.031</td> <td> 0.976</td> <td>   -0.229</td> <td>    0.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JOB_ProfExe</th>      <td>    0.0853</td> <td>    0.133</td> <td>    0.643</td> <td> 0.520</td> <td>   -0.174</td> <td>    0.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JOB_Sales</th>        <td>    1.0241</td> <td>    0.261</td> <td>    3.925</td> <td> 0.000</td> <td>    0.513</td> <td>    1.536</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>JOB_Self</th>         <td>    0.6369</td> <td>    0.212</td> <td>    3.005</td> <td> 0.003</td> <td>    0.221</td> <td>    1.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DELINQcat_1</th>      <td>    1.1760</td> <td>    0.104</td> <td>   11.334</td> <td> 0.000</td> <td>    0.973</td> <td>    1.379</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>DELINQcat_1+</th>     <td>    2.1360</td> <td>    0.112</td> <td>   19.108</td> <td> 0.000</td> <td>    1.917</td> <td>    2.355</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 5960\n",
       "Model:                          Logit   Df Residuals:                     5941\n",
       "Method:                           MLE   Df Model:                           18\n",
       "Date:                Sun, 04 Sep 2022   Pseudo R-squ.:                  0.2103\n",
       "Time:                        13:47:48   Log-Likelihood:                -2351.8\n",
       "converged:                       True   LL-Null:                       -2978.2\n",
       "Covariance Type:            nonrobust   LLR p-value:                5.480e-255\n",
       "====================================================================================\n",
       "                       coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------\n",
       "const               -1.1279      0.134     -8.389      0.000      -1.391      -0.864\n",
       "LOAN                -0.1706      0.042     -4.018      0.000      -0.254      -0.087\n",
       "MORTDUE             -0.1583      0.067     -2.366      0.018      -0.289      -0.027\n",
       "VALUE                0.0619      0.070      0.885      0.376      -0.075       0.199\n",
       "YOJ                 -0.0619      0.041     -1.510      0.131      -0.142       0.018\n",
       "CLAGE               -0.4503      0.043    -10.520      0.000      -0.534      -0.366\n",
       "NINQ                 0.2806      0.035      7.944      0.000       0.211       0.350\n",
       "CLNO                -0.1448      0.042     -3.434      0.001      -0.227      -0.062\n",
       "DEBTINC              0.4379      0.044     10.033      0.000       0.352       0.523\n",
       "DEROGzero           -1.1282      0.097    -11.653      0.000      -1.318      -0.938\n",
       "REASON_HomeImp       0.2465      0.086      2.878      0.004       0.079       0.414\n",
       "REASON_IsMissing     0.3610      0.199      1.812      0.070      -0.030       0.751\n",
       "JOB_Office          -0.6247      0.148     -4.219      0.000      -0.915      -0.335\n",
       "JOB_Other           -0.0035      0.115     -0.031      0.976      -0.229       0.222\n",
       "JOB_ProfExe          0.0853      0.133      0.643      0.520      -0.174       0.345\n",
       "JOB_Sales            1.0241      0.261      3.925      0.000       0.513       1.536\n",
       "JOB_Self             0.6369      0.212      3.005      0.003       0.221       1.052\n",
       "DELINQcat_1          1.1760      0.104     11.334      0.000       0.973       1.379\n",
       "DELINQcat_1+         2.1360      0.112     19.108      0.000       1.917       2.355\n",
       "====================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kV1SAbWTVf24"
   },
   "source": [
    "Spend a little time on inspecting the table. The pseudo $R^2$ statistic suggests that the model explains only a moderate amount of the variability of the target variable. However, we find from the *LLR test* that the model as a whole is highly significant. Further, most features come out as significant,  two dummy variables for the job categories *prof. executive* and *other* and the feature VALUE being an exception. Remember the result of the analysis of feature correlation in [Tutorial 4](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/4_nb_data_preparation.ipynb)? The result for the VALUE feature is surely due to the high correlation of this feature with MORTDUE.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I7QbSZ8oVf25",
    "outputId": "33d2a797-b504-46a5-f16e-749b87d3e747"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.82513958],\n",
       "       [0.82513958, 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(df.MORTDUE, df.VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LP4_ZmzVf29"
   },
   "source": [
    "Reestimating the logit model after removing the feature MORTDUE would be a good exercise. After the modification, VALUE should come out as significant.\n",
    "\n",
    "We could easily continue our discussion of the regression table to draw more conclusions about our data. However, this is not the point the focal tutorial. Instead, let's look at another implementation of logistic regression. The famous `scikit-learn` library is the goto library for machine learning in Python. We will use it intensively throughout the course. It also supplies an implementation of logistic regression. Let's see how to use that implementation      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ij5VpjkVf2-",
    "outputId": "5263fbdd-3bd3-4b9b-d820-285c187bcdbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(fit_intercept=False, penalty='none')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sk_logit = LogisticRegression(penalty='none', fit_intercept=False)  # Recall that we already included a constant column in X. \n",
    "sk_logit.fit(X, y)                                                  # So we must set fit_intercept to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcCMcjQLVf3C"
   },
   "source": [
    "The sklearn implementation is geared toward predictive modeling. For example, there is no way to easily obtain fitting statistics or shed light on the significance of features. On the other hand, the class `LogisticRegression` supports many advanced features to maximize the predictive power of the logit model. The above output already hints at the many different parameters that you can configure. Have a look into the help if you'd like to learn more. We will cover important parameters like those associated with *regularization* in[ Tutorial 7](https://github.com/Humboldt-WI/bads/blob/master/tutorials/7_nb_model_selection.ipynb).\n",
    "\n",
    "One easy to appreciate advantage of `sklearn` over `statsmodels` when it comes to logistic regression is scalability. The sklearn implementation is faster and can cope with larger data sets. Given the differences between our two logit models, the only way to compare them (for now) is by inspecting the estimated model coefficients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 623
    },
    "id": "ZPyDAs0GVf3D",
    "outputId": "f4187545-d384-49bb-ec01-bbaacd86b2d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statsmodels</th>\n",
       "      <th>sklearn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.127897</td>\n",
       "      <td>-1.127687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.170567</td>\n",
       "      <td>-0.170544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.158264</td>\n",
       "      <td>-0.158239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061895</td>\n",
       "      <td>0.061884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.061869</td>\n",
       "      <td>-0.061875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.450325</td>\n",
       "      <td>-0.450316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.280649</td>\n",
       "      <td>0.280644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.144770</td>\n",
       "      <td>-0.144765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.437894</td>\n",
       "      <td>0.437879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.128196</td>\n",
       "      <td>-1.128196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.246536</td>\n",
       "      <td>0.246577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.360981</td>\n",
       "      <td>0.361021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.624715</td>\n",
       "      <td>-0.625030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.003526</td>\n",
       "      <td>-0.003705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.085254</td>\n",
       "      <td>0.084992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.024129</td>\n",
       "      <td>1.024629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.636907</td>\n",
       "      <td>0.636743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.176022</td>\n",
       "      <td>1.175979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.135960</td>\n",
       "      <td>2.135871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    statsmodels   sklearn\n",
       "0     -1.127897 -1.127687\n",
       "1     -0.170567 -0.170544\n",
       "2     -0.158264 -0.158239\n",
       "3      0.061895  0.061884\n",
       "4     -0.061869 -0.061875\n",
       "5     -0.450325 -0.450316\n",
       "6      0.280649  0.280644\n",
       "7     -0.144770 -0.144765\n",
       "8      0.437894  0.437879\n",
       "9     -1.128196 -1.128196\n",
       "10     0.246536  0.246577\n",
       "11     0.360981  0.361021\n",
       "12    -0.624715 -0.625030\n",
       "13    -0.003526 -0.003705\n",
       "14     0.085254  0.084992\n",
       "15     1.024129  1.024629\n",
       "16     0.636907  0.636743\n",
       "17     1.176022  1.175979\n",
       "18     2.135960  2.135871"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe for the comparison\n",
    "# Note that we also need some conversions to ensure that the two coefficient arrays are compatible\n",
    "df_comparison = pd.DataFrame({\"statsmodels\" :logit_model.params.to_numpy(), \"sklearn\": sk_logit.coef_[0]})\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tybUfadmVf3H"
   },
   "source": [
    "**Conclusion:** the two implementations of the logit model deliver similar results. That is nice but maybe not too exciting. The main take-away is that, when it comes to logistic regression, ask yourself whether you want to use the model for an explanatory or predictive purpose. Use the implementation within `statsmodels` in the former and `sklearn` in the latter case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-VlKNTmVf3H"
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision trees are useful and powerful algorithms for classifying and regressing data. They work by on recursive partitioning which will be shown algorithmically in this notebook. Though most of the the time, sklearn can be used to implement this machine learning method, it is useful to take a look at the inner workings of tree-based algorithms. The following parts will first introduce a decision tree from scratch, before demonstrating its coverage in sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjOnBPEXfHK0"
   },
   "source": [
    "## Decision Tree From Scratch Using HMEQ Data\n",
    "\n",
    "Firstly, we will go through a decision tree made from scratch. The following code is based on the material originally created by Sebastian Mantey for the Iris Dataset whose GitHub is here: https://github.com/SebastianMantey/Decision-Tree-from-Scratch. You can also search his video on YouTube. The code has been adapted to this course's format and fit to HMEQ Dataset.\n",
    "\n",
    "Just in case you made changes to the data by running the above code, we first reload the data to a fixed and clear-defined starting point for our tree classifier. Recall that we defined the variable `data_url` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "I2zyH3v5fHK0",
    "outputId": "c08c64f0-4df5-4e58-ad9d-972c6d896d20"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BAD</th>\n",
       "      <th>LOAN</th>\n",
       "      <th>MORTDUE</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>CLAGE</th>\n",
       "      <th>NINQ</th>\n",
       "      <th>CLNO</th>\n",
       "      <th>DEBTINC</th>\n",
       "      <th>DEROGzero</th>\n",
       "      <th>REASON_HomeImp</th>\n",
       "      <th>REASON_IsMissing</th>\n",
       "      <th>JOB_Office</th>\n",
       "      <th>JOB_Other</th>\n",
       "      <th>JOB_ProfExe</th>\n",
       "      <th>JOB_Sales</th>\n",
       "      <th>JOB_Self</th>\n",
       "      <th>DELINQcat_1</th>\n",
       "      <th>DELINQcat_1+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.832283</td>\n",
       "      <td>-1.295882</td>\n",
       "      <td>-1.335526</td>\n",
       "      <td>0.266788</td>\n",
       "      <td>-1.075278</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-1.297476</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.810666</td>\n",
       "      <td>-0.013474</td>\n",
       "      <td>-0.672699</td>\n",
       "      <td>-0.236615</td>\n",
       "      <td>-0.723092</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.756608</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.789048</td>\n",
       "      <td>-1.654549</td>\n",
       "      <td>-1.839275</td>\n",
       "      <td>-0.668103</td>\n",
       "      <td>-0.368769</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-1.189302</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>-1.789048</td>\n",
       "      <td>-0.159552</td>\n",
       "      <td>-0.202559</td>\n",
       "      <td>-0.236615</td>\n",
       "      <td>-0.061033</td>\n",
       "      <td>-0.065054</td>\n",
       "      <td>-0.107566</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>-1.767431</td>\n",
       "      <td>0.791699</td>\n",
       "      <td>0.311107</td>\n",
       "      <td>-0.811933</td>\n",
       "      <td>-1.088528</td>\n",
       "      <td>-0.826792</td>\n",
       "      <td>-0.756608</td>\n",
       "      <td>0.137456</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         BAD      LOAN   MORTDUE     VALUE       YOJ     CLAGE      NINQ  \\\n",
       "index                                                                      \n",
       "0       True -1.832283 -1.295882 -1.335526  0.266788 -1.075278 -0.065054   \n",
       "1       True -1.810666 -0.013474 -0.672699 -0.236615 -0.723092 -0.826792   \n",
       "2       True -1.789048 -1.654549 -1.839275 -0.668103 -0.368769 -0.065054   \n",
       "3       True -1.789048 -0.159552 -0.202559 -0.236615 -0.061033 -0.065054   \n",
       "4      False -1.767431  0.791699  0.311107 -0.811933 -1.088528 -0.826792   \n",
       "\n",
       "           CLNO   DEBTINC  DEROGzero  REASON_HomeImp  REASON_IsMissing  \\\n",
       "index                                                                    \n",
       "0     -1.297476  0.137456       True               1                 0   \n",
       "1     -0.756608  0.137456       True               1                 0   \n",
       "2     -1.189302  0.137456       True               1                 0   \n",
       "3     -0.107566  0.137456       True               0                 1   \n",
       "4     -0.756608  0.137456       True               1                 0   \n",
       "\n",
       "       JOB_Office  JOB_Other  JOB_ProfExe  JOB_Sales  JOB_Self  DELINQcat_1  \\\n",
       "index                                                                         \n",
       "0               0          1            0          0         0            0   \n",
       "1               0          1            0          0         0            0   \n",
       "2               0          1            0          0         0            0   \n",
       "3               0          1            0          0         0            0   \n",
       "4               1          0            0          0         0            0   \n",
       "\n",
       "       DELINQcat_1+  \n",
       "index                \n",
       "0                 0  \n",
       "1                 1  \n",
       "2                 0  \n",
       "3                 0  \n",
       "4                 0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/hmeq_modeling.csv', header = 0, index_col = 0) # import file with header and index as first row and column respectively\n",
    "\n",
    "df.head() #inspect data to make sure it looks correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xQwXkg0SfHK1"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['BAD'], axis=1) #code the variables in the most standard way for your usage\n",
    "y = df[['BAD']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7tOKwVtfHK1",
    "outputId": "e89628b3-5c80-4082-f207-5f522b9988ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.frame.DataFrame'> (5960, 18) (5960, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(X), type(y), X.shape, y.shape) # double check that types and dimensions are correct before proceeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24iH_u3ANDol"
   },
   "source": [
    "## Decision Trees with sklearn\n",
    "\n",
    "As mentioned, the sklean library supports tree-based algorithms, which are a lot easier to use compared to coding a tree algorithm from scratch. Armed with the knowledge of the inner workings of a decision tree, it should be easier to understand what this algorithm is trying to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "y4Fd0cChNDol"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_split=2, max_depth=2) #keep tree at a low depth\n",
    "\n",
    "dt_shallow = clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "v_KPNLD6NDon"
   },
   "outputs": [],
   "source": [
    "pred_dt_shallow = dt_shallow.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6CTpH7yNDoq",
    "outputId": "7a067141-9d7b-46e2-8012-40c665fed606"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dt_shallow[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJT5XZOsNDos",
    "outputId": "0a98168a-6fa4-4577-a49f-f3071bc1f300"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851006711409396"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_correct_shallow = pred_dt_shallow == y.iloc[:,0]\n",
    "accuracy_shallow = classify_correct_shallow.mean()\n",
    "\n",
    "\n",
    "accuracy_shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "L7n0bgMhNDow",
    "outputId": "b21a42c3-da58-4174-83c6-8dd0cea9ce27"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(167.4, 181.2, 'X[7] <= 0.133\\nentropy = 0.721\\nsamples = 5960\\nvalue = [4771, 1189]'),\n",
       " Text(83.7, 108.72, 'X[17] <= 0.5\\nentropy = 0.315\\nsamples = 2341\\nvalue = [2208, 133]'),\n",
       " Text(41.85, 36.23999999999998, 'entropy = 0.261\\nsamples = 2215\\nvalue = [2117, 98]'),\n",
       " Text(125.55000000000001, 36.23999999999998, 'entropy = 0.852\\nsamples = 126\\nvalue = [91, 35]'),\n",
       " Text(251.10000000000002, 108.72, 'X[7] <= 0.137\\nentropy = 0.871\\nsamples = 3619\\nvalue = [2563, 1056]'),\n",
       " Text(209.25, 36.23999999999998, 'entropy = 0.959\\nsamples = 1273\\nvalue = [486, 787]'),\n",
       " Text(292.95, 36.23999999999998, 'entropy = 0.514\\nsamples = 2346\\nvalue = [2077, 269]')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tree.plot_tree(dt_shallow, fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8I98tRbNDoz"
   },
   "source": [
    "### SKLearn Tree with No Limit on Depth\n",
    "\n",
    "If we remove the maximum depth parameter, the algorithm will go as far as it can until all nodes are pure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "y4HfrTLyNDoz"
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_split=2)\n",
    "\n",
    "dt_deep = clf.fit(X, y)\n",
    "\n",
    "pred_dt_deep = dt_deep.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Na0BNJFNDo1",
    "outputId": "3f18be46-1f98-47d2-efc1-2ef28821a90c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True, False])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dt_deep[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s-rfG-hbNDo3",
    "outputId": "3580db64-b812-43bf-851c-2036da9384ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_correct_deep = pred_dt_deep == y.iloc[:,0]\n",
    "accuracy_deep = classify_correct_deep.mean()\n",
    "\n",
    "accuracy_deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs1jcqGTNDo5"
   },
   "source": [
    "While this seems great at first glance, what is the problem here? What does this mean for new data that we encounter whose idiosyncracies may be slightly different than the data we used to train the model?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HUkvWfFsVf2m",
    "WGbaWjvgNDna",
    "rDBY9KhyNDns",
    "3m-msruhNDoB",
    "tXwvb-StNDoE",
    "B0xBW-wxNDoH",
    "zG3DmknMNDoL",
    "aaDC9YXTNDoU",
    "B4eGRMz_NDob",
    "24iH_u3ANDol",
    "Q8I98tRbNDoz"
   ],
   "name": "5_nb_supervised_learningmodellingreview.ipynb.txt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
